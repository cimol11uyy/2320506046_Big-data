{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2eb1263-b9b3-41fa-a78d-9e95c033d807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark==3.4.1 in c:\\users\\hp\\anaconda3\\envs\\praktikumbigdata\\lib\\site-packages (3.4.1)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\hp\\anaconda3\\envs\\praktikumbigdata\\lib\\site-packages (from pyspark==3.4.1) (0.10.9.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~yspark (C:\\Users\\hp\\anaconda3\\envs\\praktikumbigdata\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~yspark (C:\\Users\\hp\\anaconda3\\envs\\praktikumbigdata\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~yspark (C:\\Users\\hp\\anaconda3\\envs\\praktikumbigdata\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "  pip install pyspark==3.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66cff26b-b8ed-4399-bbc0-52f785269945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\hp\\anaconda3\\envs\\praktikumbigdata\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\hp\\anaconda3\\envs\\praktikumbigdata\\lib\\site-packages (from pandas) (2.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hp\\anaconda3\\envs\\praktikumbigdata\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hp\\anaconda3\\envs\\praktikumbigdata\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\hp\\anaconda3\\envs\\praktikumbigdata\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hp\\anaconda3\\envs\\praktikumbigdata\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~yspark (C:\\Users\\hp\\anaconda3\\envs\\praktikumbigdata\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~yspark (C:\\Users\\hp\\anaconda3\\envs\\praktikumbigdata\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~yspark (C:\\Users\\hp\\anaconda3\\envs\\praktikumbigdata\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90bf962b-40cc-4b80-92ba-88007e465c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: findspark in c:\\users\\hp\\anaconda3\\envs\\praktikumbigdata\\lib\\site-packages (2.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~yspark (C:\\Users\\hp\\anaconda3\\envs\\praktikumbigdata\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~yspark (C:\\Users\\hp\\anaconda3\\envs\\praktikumbigdata\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~yspark (C:\\Users\\hp\\anaconda3\\envs\\praktikumbigdata\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e840be6b-d77a-4c26-8126-401225c3bcbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.1.0 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"C:\\Users\\hp\\anaconda3\\envs\\praktikumbigdata\\Lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\hp\\anaconda3\\envs\\praktikumbigdata\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\hp\\anaconda3\\envs\\praktikumbigdata\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 701, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\hp\\anaconda3\\envs\\praktikumbigdata\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\hp\\anaconda3\\envs\\praktikumbigdata\\Lib\\asyncio\\windows_events.py\", line 321, in run_forever\n",
      "    super().run_forever()\n",
      "  File \"C:\\Users\\hp\\anaconda3\\envs\\praktikumbigdata\\Lib\\asyncio\\base_events.py\", line 608, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\hp\\anaconda3\\envs\\praktikumbigdata\\Lib\\asyncio\\base_events.py\", line 1936, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\hp\\anaconda3\\envs\\praktikumbigdata\\Lib\\asyncio\\events.py\", line 84, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\hp\\anaconda3\\envs\\praktikumbigdata\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"C:\\Users\\hp\\anaconda3\\envs\\praktikumbigdata\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 523, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"C:\\Users\\hp\\anaconda3\\envs\\praktikumbigdata\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 429, in dispatch_shell\n",
      "    await result\n",
      "  File \"C:\\Users\\hp\\anaconda3\\envs\\praktikumbigdata\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"C:\\Users\\hp\\anaconda3\\envs\\praktikumbigdata\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 429, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"C:\\Users\\hp\\anaconda3\\envs\\praktikumbigdata\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\hp\\anaconda3\\envs\\praktikumbigdata\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"C:\\Users\\hp\\anaconda3\\envs\\praktikumbigdata\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"C:\\Users\\hp\\anaconda3\\envs\\praktikumbigdata\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\hp\\anaconda3\\envs\\praktikumbigdata\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"C:\\Users\\hp\\anaconda3\\envs\\praktikumbigdata\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"C:\\Users\\hp\\anaconda3\\envs\\praktikumbigdata\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_11200\\3451764375.py\", line 24, in <module>\n",
      "    df = spark.createDataFrame(data, schema=columns)\n",
      "  File \"C:\\spark-3.5.2-bin-hadoop3\\python\\pyspark\\sql\\session.py\", line 1397, in createDataFrame\n",
      "    import pandas as pd\n",
      "  File \"C:\\Users\\hp\\anaconda3\\envs\\praktikumbigdata\\Lib\\site-packages\\pandas\\__init__.py\", line 49, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"C:\\Users\\hp\\anaconda3\\envs\\praktikumbigdata\\Lib\\site-packages\\pandas\\core\\api.py\", line 1, in <module>\n",
      "    from pandas._libs import (\n",
      "  File \"C:\\Users\\hp\\anaconda3\\envs\\praktikumbigdata\\Lib\\site-packages\\pandas\\_libs\\__init__.py\", line 17, in <module>\n",
      "    import pandas._libs.pandas_datetime  # noqa: F401 # isort: skip # type: ignore[reportUnusedImport]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+------+\n",
      "|EmployeeName|Department|Salary|\n",
      "+------------+----------+------+\n",
      "|       James|     Sales|  3000|\n",
      "|     Michael|     Sales|  4600|\n",
      "|      Robert|     Sales|  4100|\n",
      "|       Maria|   Finance|  3000|\n",
      "+------------+----------+------+\n",
      "\n",
      "+------------+------+\n",
      "|EmployeeName|Salary|\n",
      "+------------+------+\n",
      "|       James|  3000|\n",
      "|     Michael|  4600|\n",
      "|      Robert|  4100|\n",
      "|       Maria|  3000|\n",
      "+------------+------+\n",
      "\n",
      "+------------+----------+------+\n",
      "|EmployeeName|Department|Salary|\n",
      "+------------+----------+------+\n",
      "|     Michael|     Sales|  4600|\n",
      "|      Robert|     Sales|  4100|\n",
      "+------------+----------+------+\n",
      "\n",
      "+----------+-----------+\n",
      "|Department|avg(Salary)|\n",
      "+----------+-----------+\n",
      "|     Sales|     3900.0|\n",
      "|   Finance|     3000.0|\n",
      "+----------+-----------+\n",
      "\n",
      "+----------+-----------+\n",
      "|Department|max(Salary)|\n",
      "+----------+-----------+\n",
      "|     Sales|       4600|\n",
      "|   Finance|       3000|\n",
      "+----------+-----------+\n",
      "\n",
      "+----------+-----------+\n",
      "|Department|sum(Salary)|\n",
      "+----------+-----------+\n",
      "|     Sales|      11700|\n",
      "|   Finance|       3000|\n",
      "+----------+-----------+\n",
      "\n",
      "+------------+----------+------+-----------+\n",
      "|EmployeeName|Department|Salary|SalaryBonus|\n",
      "+------------+----------+------+-----------+\n",
      "|       James|     Sales|  3000|      300.0|\n",
      "|     Michael|     Sales|  4600|      460.0|\n",
      "|      Robert|     Sales|  4100|      410.0|\n",
      "|       Maria|   Finance|  3000|      300.0|\n",
      "+------------+----------+------+-----------+\n",
      "\n",
      "+------------+----------+------+----------------+\n",
      "|EmployeeName|Department|Salary|karyawan_terbaik|\n",
      "+------------+----------+------+----------------+\n",
      "|       James|     Sales|  3000|           false|\n",
      "|     Michael|     Sales|  4600|           false|\n",
      "|      Robert|     Sales|  4100|           false|\n",
      "|       Maria|   Finance|  3000|            true|\n",
      "+------------+----------+------+----------------+\n",
      "\n",
      "+------------+----------+------+----+\n",
      "|EmployeeName|Department|Salary|Rank|\n",
      "+------------+----------+------+----+\n",
      "|       Maria|   Finance|  3000|   1|\n",
      "|       James|     Sales|  3000|   1|\n",
      "|      Robert|     Sales|  4100|   2|\n",
      "|     Michael|     Sales|  4600|   3|\n",
      "+------------+----------+------+----+\n",
      "\n",
      "+------------+----------+------+-------------+\n",
      "|EmployeeName|Department|Salary|kategori gaji|\n",
      "+------------+----------+------+-------------+\n",
      "|       James|     Sales|  3000|          Low|\n",
      "|     Michael|     Sales|  4600|         High|\n",
      "|      Robert|     Sales|  4100|         High|\n",
      "|       Maria|   Finance|  3000|          Low|\n",
      "+------------+----------+------+-------------+\n",
      "\n",
      "+------------+----------+------+----+-------------+\n",
      "|EmployeeName|Department|Salary|Rank|kategori gaji|\n",
      "+------------+----------+------+----+-------------+\n",
      "|       Maria|   Finance|  3000|   1|          Low|\n",
      "|       James|     Sales|  3000|   1|          Low|\n",
      "|      Robert|     Sales|  4100|   2|         High|\n",
      "|     Michael|     Sales|  4600|   3|         High|\n",
      "+------------+----------+------+----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "spark = SparkSession.builder.appName('HandsOnPertemuan6').getOrCreate()\n",
    "\n",
    "data = [('James', 'Sales', 3000),\n",
    "        ('Michael', 'Sales', 4600),\n",
    "        ('Robert', 'Sales', 4100),\n",
    "        ('Maria', 'Finance', 3000)]\n",
    "columns = ['EmployeeName', 'Department', 'Salary']\n",
    "\n",
    "def salary_category(Salary):\n",
    "    if Salary > 4000:\n",
    "        return \"High\"\n",
    "    else:\n",
    "        return \"Low\"\n",
    "\n",
    "salary_category_udf = udf(salary_category, StringType())\n",
    "\n",
    "df = spark.createDataFrame(data, schema=columns)\n",
    "df.show()\n",
    "df.select('EmployeeName', 'Salary').show()\n",
    "df.filter(df['Salary'] > 3000).show()\n",
    "df.groupBy('Department').avg('Salary').show()\n",
    "df.groupBy('Department').max('Salary').show()\n",
    "df.groupBy('Department').sum('Salary').show()\n",
    "df.withColumn('SalaryBonus', df['Salary'] * 0.1).show()\n",
    "df.withColumn('karyawan_terbaik',df['EmployeeName'] == 'Maria').show()\n",
    "windowSpec = Window.partitionBy('Department').orderBy('Salary')\n",
    "df.withColumn('Rank', F.rank().over(windowSpec)).show()\n",
    "df.withColumn(\"kategori gaji\", salary_category_udf(df['Salary'])).show()\n",
    "df_gabung = df.withColumn('Rank', F.rank().over(windowSpec))\\\n",
    ".withColumn(\"kategori gaji\", salary_category_udf(df['Salary']))\n",
    "df_gabung.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c02443-fc0f-48fd-bb86-aac6416dce49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
